{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embedding Subtitles to Videos Automatically using Python, OpenAI Whisper and FFMPEG ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Steps to be followed**\n",
    "\n",
    "1. Take a video URL of interest\n",
    "2. Download and save the video using PyTube library\n",
    "3. Print some other useful information about the video. \n",
    "4. Extract and save Audio as a .WAV file from the Video file using FFMPEG. \n",
    "5. Invoke local OpenAI Whisper model to get Transcriptions using faster-whisper.\n",
    "6. Preprocess transcripts for time format adjustments. \n",
    "7. Use FFMPEG to embed subtitles in the video. \n",
    "\n",
    "**Flow**\n",
    "\n",
    "![Visual Flow](subtitles.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pytube"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/vaibhavpandey/Desktop/generativegeek/add-subs-to-videos/Introducing GPT-4o.mp4'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "url = \"https://www.youtube.com/watch?v=DQacCB9tDaw\"\n",
    "yt = pytube.YouTube(url)\n",
    "\n",
    "yt.streams.filter(progressive=True, file_extension='mp4').order_by('resolution').desc().first().download()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: Introducing GPT-4o\n",
      "Views: 2847440\n",
      "Description: OpenAI Spring Update â€“ streamed live on Monday, May 13, 2024. \n",
      "\n",
      "Introducing GPT-4o, updates to ChatGPT, and more.\n",
      "Length: 1573 seconds\n"
     ]
    }
   ],
   "source": [
    "print(f\"Title: {yt.title}\")\n",
    "print(f\"Views: {yt.views}\")\n",
    "print(f\"Description: {yt.description}\")\n",
    "print(f\"Length: {yt.length} seconds\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#rename the mp4 to the title of teh video without a file extension\n",
    "os.rename(yt.title + \".mp4\", yt.title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract audio from the video\n",
    "import time\n",
    "import math \n",
    "import ffmpeg\n",
    "\n",
    "def extract_audio(input_file):\n",
    "    extracted_audio = f\"audio-{input_file}.wav\"\n",
    "    stream = ffmpeg.input(input_file)\n",
    "    stream = ffmpeg.output(stream, extracted_audio)\n",
    "    ffmpeg.run(stream, overwrite_output=True)\n",
    "    return extracted_audio\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ffmpeg version 7.0 Copyright (c) 2000-2024 the FFmpeg developers\n",
      "  built with Apple clang version 15.0.0 (clang-1500.3.9.4)\n",
      "  configuration: --prefix=/opt/homebrew/Cellar/ffmpeg/7.0_1 --enable-shared --enable-pthreads --enable-version3 --cc=clang --host-cflags= --host-ldflags='-Wl,-ld_classic' --enable-ffplay --enable-gnutls --enable-gpl --enable-libaom --enable-libaribb24 --enable-libbluray --enable-libdav1d --enable-libharfbuzz --enable-libjxl --enable-libmp3lame --enable-libopus --enable-librav1e --enable-librist --enable-librubberband --enable-libsnappy --enable-libsrt --enable-libssh --enable-libsvtav1 --enable-libtesseract --enable-libtheora --enable-libvidstab --enable-libvmaf --enable-libvorbis --enable-libvpx --enable-libwebp --enable-libx264 --enable-libx265 --enable-libxml2 --enable-libxvid --enable-lzma --enable-libfontconfig --enable-libfreetype --enable-frei0r --enable-libass --enable-libopencore-amrnb --enable-libopencore-amrwb --enable-libopenjpeg --enable-libopenvino --enable-libspeex --enable-libsoxr --enable-libzmq --enable-libzimg --disable-libjack --disable-indev=jack --enable-videotoolbox --enable-audiotoolbox --enable-neon\n",
      "  libavutil      59.  8.100 / 59.  8.100\n",
      "  libavcodec     61.  3.100 / 61.  3.100\n",
      "  libavformat    61.  1.100 / 61.  1.100\n",
      "  libavdevice    61.  1.100 / 61.  1.100\n",
      "  libavfilter    10.  1.100 / 10.  1.100\n",
      "  libswscale      8.  1.100 /  8.  1.100\n",
      "  libswresample   5.  1.100 /  5.  1.100\n",
      "  libpostproc    58.  1.100 / 58.  1.100\n",
      "Input #0, mov,mp4,m4a,3gp,3g2,mj2, from 'Introducing GPT-4o':\n",
      "  Metadata:\n",
      "    major_brand     : mp42\n",
      "    minor_version   : 0\n",
      "    compatible_brands: isommp42\n",
      "    creation_time   : 2024-05-13T17:40:47.000000Z\n",
      "    encoder         : Google\n",
      "  Duration: 00:26:12.64, start: 0.000000, bitrate: 827 kb/s\n",
      "  Stream #0:0[0x1](und): Video: h264 (High) (avc1 / 0x31637661), yuv420p(tv, bt709, progressive), 1280x720 [SAR 1:1 DAR 16:9], 695 kb/s, 30 fps, 30 tbr, 15360 tbn (default)\n",
      "      Metadata:\n",
      "        creation_time   : 2024-05-13T17:40:47.000000Z\n",
      "        handler_name    : ISO Media file produced by Google Inc. Created on: 05/13/2024.\n",
      "        vendor_id       : [0][0][0][0]\n",
      "  Stream #0:1[0x2](und): Audio: aac (LC) (mp4a / 0x6134706D), 44100 Hz, stereo, fltp, 128 kb/s (default)\n",
      "      Metadata:\n",
      "        creation_time   : 2024-05-13T17:40:47.000000Z\n",
      "        handler_name    : ISO Media file produced by Google Inc. Created on: 05/13/2024.\n",
      "        vendor_id       : [0][0][0][0]\n",
      "Stream mapping:\n",
      "  Stream #0:1 -> #0:0 (aac (native) -> pcm_s16le (native))\n",
      "Press [q] to stop, [?] for help\n",
      "Output #0, wav, to 'audio-Introducing GPT-4o.wav':\n",
      "  Metadata:\n",
      "    major_brand     : mp42\n",
      "    minor_version   : 0\n",
      "    compatible_brands: isommp42\n",
      "    ISFT            : Lavf61.1.100\n",
      "  Stream #0:0(und): Audio: pcm_s16le ([1][0][0][0] / 0x0001), 44100 Hz, stereo, s16, 1411 kb/s (default)\n",
      "      Metadata:\n",
      "        creation_time   : 2024-05-13T17:40:47.000000Z\n",
      "        handler_name    : ISO Media file produced by Google Inc. Created on: 05/13/2024.\n",
      "        vendor_id       : [0][0][0][0]\n",
      "        encoder         : Lavc61.3.100 pcm_s16le\n",
      "[out#0/wav @ 0x13fa044d0] video:0KiB audio:270912KiB subtitle:0KiB other streams:0KiB global headers:0KiB muxing overhead: 0.000028%\n",
      "size=  270912KiB time=00:26:12.64 bitrate=1411.2kbits/s speed=1.22e+03x    \n"
     ]
    }
   ],
   "source": [
    "audio_extract = extract_audio(yt.title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transcribe the audio\n",
    "from faster_whisper import WhisperModel\n",
    "def transcribe(audio):\n",
    "\n",
    "    model = WhisperModel(\"small\")\n",
    "    segments, info = model.transcribe(audio)\n",
    "    language = info[0]\n",
    "    print(f\" Transcription Language: {language}\")\n",
    "    segments = list(segments) # this is where the transcribe happens\n",
    "\n",
    "    for segment in segments: \n",
    "        print(\"[%.2fs -> %.2fs] %s\" % (segment.start, segment.end, segment.text))\n",
    "    return language, segments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-05-15 15:28:59.583] [ctranslate2] [thread 1308885] [warning] The compute type inferred from the saved model is float16, but the target device or backend do not support efficient float16 computation. The model weights have been automatically converted to use the float32 compute type instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Transcription Language: en\n",
      "[0.00s -> 20.96s]  Hi everyone, thank you, thank you, it's great to have you here today.\n",
      "[20.96s -> 26.88s]  Today I'm going to talk about three things, that's it, we will start with why it's so\n",
      "[26.88s -> 32.80s]  important to us to have a product that we can make freely available and broadly available to\n",
      "[32.80s -> 39.52s]  everyone and we're always trying to find out ways to reduce friction so everyone can use\n",
      "[39.52s -> 46.40s]  Chagivity wherever they are. So today we'll be releasing the desktop version of Chagivity\n",
      "[46.40s -> 54.56s]  and the refreshed UI that makes it simpler to use much more natural as well. But the big news today\n",
      "[54.56s -> 62.80s]  is that we are launching our new flagship model and we are calling it GPT-4O. The special thing\n",
      "[62.80s -> 70.88s]  about GPT-4O is that it brings GPT-4 level intelligence to everyone including our free users.\n",
      "[71.84s -> 77.68s]  We'll be showing some live demos today to show the full extent of the capabilities of our new\n",
      "[77.68s -> 85.44s]  model and we'll be rolling them out iteratively over the next few weeks. All right so let's get\n",
      "[85.44s -> 94.16s]  started. A very important part of our mission is to be able to make our advanced AI tools available\n",
      "[94.16s -> 101.04s]  to everyone for free. We think it's very very important that people have an intuitive feel\n",
      "[101.04s -> 106.88s]  for what the technology can do and so we really want to pair it with this broader\n",
      "[106.88s -> 113.04s]  understanding and we're always finding ways to reduce that friction and recently we made\n",
      "[113.04s -> 120.88s]  Chagivity available without the sign-up flow and today we're also bringing the desktop app to\n",
      "[120.88s -> 127.60s]  Chagivity because we want you to be able to use it wherever you are. As you can see it's easy,\n",
      "[127.60s -> 135.76s]  it's simple, it integrates very very easily in your workflow. Along with it we have also refreshed\n",
      "[135.76s -> 144.72s]  the UI. We know that these models get more and more complex but we want the experience of interaction\n",
      "[144.72s -> 152.48s]  to actually become more natural, easy and for you not to focus on the UI at all but just focus on\n",
      "[152.48s -> 162.32s]  the collaboration which had GPT. And now the big news. Today we are releasing our newest flagship\n",
      "[162.32s -> 178.32s]  model. This is GPT4O. GPT4O provides GPT4 level intelligence but it is much faster\n",
      "[179.12s -> 187.28s]  and it improves on its capabilities across text, vision and audio. For the past couple of years\n",
      "[187.28s -> 192.88s]  we've been very focused on improving the intelligence of these models and they've gotten pretty good\n",
      "[194.00s -> 199.28s]  but this is the first time that we are really making a huge step forward when it comes to\n",
      "[199.28s -> 207.44s]  the ease of use and this is incredibly important because we're looking at the future of interaction\n",
      "[207.44s -> 216.00s]  between ourselves and the machines and we think that GPT4O is really shifting that paradigm\n",
      "[216.00s -> 221.76s]  into the future of collaboration where this interaction becomes much more natural and far\n",
      "[221.76s -> 228.72s]  far easier. But you know making this happen is actually quite complex because when we interact\n",
      "[228.72s -> 235.52s]  with one another there's a lot of stuff that we take for granted. You know the ease of our dialogue\n",
      "[235.52s -> 242.32s]  when we interrupt one another, the background noises, the multiple voices in a conversation\n",
      "[242.32s -> 248.88s]  or you know understanding the tone of voice. All of these things are actually quite complex\n",
      "[248.88s -> 256.96s]  for for these models and until now with voice mode we had three models that come together\n",
      "[256.96s -> 263.92s]  to deliver this experience. We have transcription, intelligence and then text to speech. All comes\n",
      "[263.92s -> 272.24s]  together in orchestration to deliver voice mode. This also brings a lot of latency to the experience.\n",
      "[272.32s -> 280.40s]  And it really breaks that immersion in the collaboration which had GPT. But now with GPT4O\n",
      "[280.40s -> 291.68s]  this all happens natively. GPT4O reasons across voice, text and vision. And with these incredible\n",
      "[291.68s -> 299.04s]  efficiencies it also allows us to bring the GPT4 class intelligence to our free users.\n",
      "[299.68s -> 305.44s]  This is something that we've been trying to do for many many months and we're very very excited\n",
      "[305.44s -> 315.76s]  to finally bring GPT4O to all of our users. To date we have 100 million people, more than 100\n",
      "[315.76s -> 325.84s]  million in fact. They use chat GPT to create, work, learn. And we have these advanced tools that\n",
      "[326.48s -> 332.96s]  are only available to our paid users at least until now. With the efficiencies of 4O we can\n",
      "[332.96s -> 340.88s]  bring these tools to everyone. So starting today you can use GPTs in the GPT store.\n",
      "[342.88s -> 350.00s]  So far we've had more than a million users create amazing experiences with GPTs. These are custom\n",
      "[350.00s -> 356.80s]  chat GPTs for specific use cases that are available in the store. And now our builders have a much\n",
      "[356.80s -> 364.00s]  bigger audience where university professors can create content for their students or podcasters\n",
      "[364.00s -> 372.88s]  can create content for their listeners. And you can also use vision. So now you can upload\n",
      "[373.68s -> 381.84s]  screenshots, photos, documents containing both text and images and you can start conversations\n",
      "[381.84s -> 389.20s]  with chat GPT about all of this content. You can also use memory where it makes chat GPT far more\n",
      "[389.20s -> 394.88s]  useful and helpful because now it has a sense of continuity across of all your conversations.\n",
      "[395.92s -> 401.12s]  And you can use browse where you can search for real-time information in your conversation\n",
      "[401.68s -> 408.32s]  and advanced data analysis where you can upload charts or any information and it will analyze\n",
      "[409.68s -> 413.12s]  this information. It will give you answers and so on.\n",
      "[415.52s -> 422.96s]  Lastly, we have also improved on the quality and speed in 50 different languages\n",
      "[423.92s -> 429.84s]  for chat GPT. And this is very, very important because we want to be able to bring this experience\n",
      "[429.84s -> 437.28s]  to as many people out there as possible. So we're very, very excited to bring GPT 4.0\n",
      "[437.92s -> 443.04s]  to all of our free users out there and for the paid users they will continue to have\n",
      "[443.92s -> 452.80s]  up to five times the capacity limits of our free users. But GPT 4.0 is not only available\n",
      "[452.80s -> 466.40s]  in chat GPT, we're also bringing it to the API. So our developers can start building today with\n",
      "[466.40s -> 475.52s]  GPT 4.0 and making amazing AI applications deploying them at scale. 4.0 is available at 2x faster,\n",
      "[475.52s -> 480.96s]  50% cheaper and five times higher rate limits compared to GPT 4.0 turbo.\n",
      "[483.76s -> 490.88s]  But, you know, as we bring these technologies into the world, it's quite challenging to figure out\n",
      "[490.88s -> 500.48s]  how to do so in a way that's both useful and also safe. And GPT 4.0 presents new challenges for us\n",
      "[500.48s -> 507.92s]  when it comes to safety because we're dealing with real-time audio, real-time vision. And our team\n",
      "[507.92s -> 516.32s]  has been hard at work figuring out how to build in mitigations against misuse. We continue to work\n",
      "[516.32s -> 521.76s]  with different stakeholders out there from government, media, entertainment, all industries,\n",
      "[521.76s -> 529.76s]  red teamers, civil society to figure out how to best bring these technologies into the world. So\n",
      "[529.76s -> 536.16s]  over the next few weeks, we'll continue our iterative deployment to bring out all the capabilities\n",
      "[536.16s -> 543.28s]  to you. But today, I want to show you all these capabilities. So we'll do some live demos.\n",
      "[544.24s -> 550.56s]  I will bring on two of our research leads, Mark Chen and Barrett Zov to join us.\n",
      "[558.72s -> 564.48s]  Hi, I'm Barrett. Hey, I'm Mark. So one of the key capabilities we're really excited to share with\n",
      "[564.48s -> 570.32s]  you today is real-time conversational speech. Let's just get a demo fired up. So I'm taking out\n",
      "[570.32s -> 576.16s]  a phone. If you are wondering about this wire, so we have consistent internet. And if you see,\n",
      "[576.16s -> 581.44s]  there's this little icon on the bottom right of the ChatGPT app, and this will open up GPT 4.0's\n",
      "[581.44s -> 592.08s]  audio capabilities. Hey, ChatGPT, I'm Mark. How are you? Oh, Mark. I'm doing great. Thanks for asking.\n",
      "[592.08s -> 597.36s]  How about you? Hey, so I'm on stage right now. I'm doing a live demo, and frankly, I'm feeling a\n",
      "[597.36s -> 603.04s]  little bit nervous. Can you help me calm my nerves a little bit? Oh, you're doing a live demo right\n",
      "[603.04s -> 610.80s]  now? That's awesome. Just take a deep breath. And remember, you're the expert. I like that\n",
      "[610.80s -> 615.52s]  suggestion. Let me try a couple of deep breaths. Can you give me feedback on my breaths? Okay, here\n",
      "[615.52s -> 630.08s]  I go. Whoa, slow. Do a bit there. Mark, you're not a vacuum cleaner. Breathe in for a count of four.\n",
      "[630.08s -> 639.84s]  Okay, let me try again. So I'm going to breathe in deeply. And then for four, and then exhale slowly.\n",
      "[640.00s -> 649.36s]  Okay, I'll try again. Breathing in and breathe out. That's it. How do you feel? I feel a lot better.\n",
      "[649.36s -> 655.92s]  Thank you so much. So Mark, you've been working on these capabilities for a while now. Can you tell\n",
      "[655.92s -> 661.20s]  us a bit how it's different from Voice Mode? Right. So if you've used our Voice Mode experience\n",
      "[661.20s -> 665.84s]  before, you'll notice a couple of key differences. First, you know, you can now interrupt the model.\n",
      "[665.84s -> 670.64s]  You don't have to wait for it to finish your turn before you can start speaking. And you know,\n",
      "[670.64s -> 676.96s]  you can just butt in whenever you want. Second, the model is real-time responsiveness. So that means\n",
      "[676.96s -> 680.96s]  that you don't have this awkward two to three second lag before you wait for the model to give a\n",
      "[680.96s -> 686.80s]  response. And finally, the model picks up on emotion, right? When I was breathing super hard there,\n",
      "[686.80s -> 691.44s]  it could tell and you, hey, you might want to calm down a little bit. You're kind of running a little\n",
      "[691.44s -> 696.56s]  bit too fast. So, you know, it really has that capability across the board to perceive your\n",
      "[696.56s -> 702.80s]  emotion. Not only that, though, the model is able to generate voice in a variety of different emotive\n",
      "[702.80s -> 708.00s]  styles. And it really has a wide dynamic range. So I'm going to fire up another demo with that.\n",
      "[708.00s -> 718.24s]  Yeah. Hey, ChatGBT, how are you doing? I'm doing fantastic. Thanks for asking. How about you?\n",
      "[718.32s -> 722.80s]  Pretty good. What's up? So my friend Barrett here, he's been having trouble sleeping lately.\n",
      "[722.80s -> 726.16s]  And I want you to tell him a bedtime story about robots and love.\n",
      "[726.96s -> 735.44s]  Oh, a bedtime story about robots and love? I got you covered. Gather round, Barrett. Once upon a\n",
      "[735.44s -> 741.84s]  time, in a world not too different from ours, there was a robot named Bite. Bite was a curious\n",
      "[741.84s -> 745.92s]  robot. Always explore- I've always started this story, but I want a little bit more emotion in\n",
      "[745.92s -> 753.36s]  your voice, a little bit more drama. Got it. Let's add some drama. Once upon a time, in a world\n",
      "[753.36s -> 758.00s]  not too different from ours, there was a robot named Bite. No, no, no, ChatGBT, I really want\n",
      "[758.00s -> 764.16s]  maximal emotion, like maximal expressiveness, much more than you were doing before. Understood.\n",
      "[764.16s -> 771.84s]  Let's amplify the drama. Once upon a time, in a world not too different from ours, there was a\n",
      "[771.84s -> 780.96s]  robot named Bite. Can you do this in a robotic voice now? Initiating dramatic robotic voice.\n",
      "[781.52s -> 788.48s]  Once upon a time, in a world not too different from ours, there was a robot named Bite.\n",
      "[789.04s -> 794.80s]  Bite was a curious robot. Always exploring new circuits. Barrett here, calm down a little bit.\n",
      "[794.80s -> 797.68s]  Can you end the story, but do it in a singing voice?\n",
      "[800.08s -> 808.40s]  And so Bite found another robot friend and may live circuitly ever after.\n",
      "[809.20s -> 812.40s]  Thank you so much. I definitely feel a lot more relaxed.\n",
      "[812.72s -> 824.08s]  You've just seen the voice capabilities, but we also want to show you the vision capabilities\n",
      "[824.08s -> 828.64s]  as the model can also see the whole world around us. Yeah, absolutely. As you can see,\n",
      "[828.64s -> 833.44s]  you know, you can chat with ChatGBT in real time, but you can also interact with it with video as\n",
      "[834.40s -> 843.68s]  well. Okay, let me boot up ChatGBT. Hey ChatGBT. Hello there. How's it going?\n",
      "[844.56s -> 847.84s]  It's going really well. Today I'd really like your help solving a math problem.\n",
      "[848.48s -> 852.00s]  I'm all ears. What math problem can I help you tackle today?\n",
      "[852.56s -> 855.28s]  So I'm going to write down a linear equation on a sheet of paper and I'll show you,\n",
      "[855.28s -> 858.80s]  and then I'd love your help working me through it. But importantly, don't tell me the solution,\n",
      "[858.80s -> 865.52s]  just help give me hints along the way. Got it. Okay, I see it.\n",
      "[866.72s -> 869.76s]  No, I didn't show you yet. Just give me help along the way. One second.\n",
      "[872.16s -> 875.60s]  Whoops, I got too excited. I'm ready when you are.\n",
      "[875.60s -> 889.12s]  So, ChatGBT, what equation did I write down there?\n",
      "[890.08s -> 896.88s]  Ah, I see it now. You wrote down 3x plus 1 equals 4.\n",
      "[896.88s -> 900.08s]  Yep, exactly. So what's the first step I should take to try to solve this?\n",
      "[900.80s -> 907.36s]  The first step is to get all the terms with x on one side and the constants on the other side.\n",
      "[907.92s -> 910.16s]  So what do you think we should do with that plus 1?\n",
      "[910.96s -> 913.68s]  Okay, I'm going to try to subtract 1 from both sides and then I'll see what I get.\n",
      "[915.28s -> 921.36s]  Great idea. Perfect. Now what do you get when you subtract 1 from both sides?\n",
      "[926.48s -> 929.68s]  Okay, so ChatGBT, this is what I ended up with. How does this look?\n",
      "[930.96s -> 937.44s]  Spot on. Now you've isolated the term with x on one side and it looks like 3x equal 3.\n",
      "[938.00s -> 940.40s]  What do you think we should do next to solve for x?\n",
      "[940.96s -> 942.40s]  I'm not sure. Can you give me another hint?\n",
      "[943.92s -> 949.28s]  Of course. You have 3x and you want to find the value of x.\n",
      "[949.28s -> 952.72s]  Think about what operation would undo multiplication.\n",
      "[952.72s -> 953.60s]  Is it subtraction?\n",
      "[954.56s -> 961.04s]  Close, but think of the opposite of multiplication because 3x means 3 times x.\n",
      "[961.92s -> 965.44s]  Okay, I think it's division. Let me try to divide both sides by 3 and let me see what I get.\n",
      "[966.32s -> 969.20s]  Bingo! Go ahead and divide both sides by 3.\n",
      "[975.28s -> 978.40s]  Okay, so ChatGBT, this is what I ended up with. How does this look?\n",
      "[979.36s -> 985.12s]  It looks perfect. You've solved it and x equals 1. Nicely done.\n",
      "[985.12s -> 987.52s]  How do you feel about solving linear equations now?\n",
      "[988.48s -> 992.64s]  Yeah, honestly, I'm not pretty confident I can solve linear equations,\n",
      "[992.64s -> 996.00s]  but why would you ever need to use this in the real world? I'm really skeptical.\n",
      "[997.20s -> 1002.40s]  That's a fair question. Linear equations pop up in a lot of everyday situations,\n",
      "[1002.40s -> 1006.80s]  even if we don't realize it. For example, calculating expenses,\n",
      "[1006.80s -> 1011.52s]  planning travel, cooking, and even in business for profit and loss calculations.\n",
      "[1012.08s -> 1016.64s]  It's basically a way to solve problems where you need to find an unknown value.\n",
      "[1016.64s -> 1020.00s]  Plus, I have a lot of unknown values in my life.\n",
      "[1020.00s -> 1023.28s]  I'm totally convinced I'm going to learn math now.\n",
      "[1026.00s -> 1031.36s]  I love that attitude. Math can definitely help solve some of life's mysteries.\n",
      "[1031.36s -> 1034.72s]  Whenever you're ready to dive into more math, I'm here for you.\n",
      "[1034.72s -> 1036.64s]  Anything else you'd like to tackle today?\n",
      "[1036.64s -> 1040.64s]  So ChatGBT, I really love that you taught the value of math to my friend Mark.\n",
      "[1040.64s -> 1043.20s]  And I wrote one last thing. I'd love if you could take a look at.\n",
      "[1043.92s -> 1048.24s]  Of course. I'd love to see what you wrote. Show it to me whenever you're ready.\n",
      "[1048.24s -> 1050.24s]  Okay, so this is what I wrote down. What do you see?\n",
      "[1053.20s -> 1057.28s]  I see. I love ChatGBT. That's so sweet of you.\n",
      "[1057.60s -> 1062.64s]  Yeah, well, I really appreciate all the help. So yeah, as we can see.\n",
      "[1067.44s -> 1070.16s]  Wow, that's quite the outfit you've got on.\n",
      "[1070.16s -> 1073.76s]  Yeah, as we can see, we can chat in real time to ChatGBT.\n",
      "[1073.76s -> 1076.32s]  And not only that, we can interact with it with video as well.\n",
      "[1077.12s -> 1081.52s]  And I swear, if you look at Barrett's GPT history, it's all equations like this.\n",
      "[1082.40s -> 1089.36s]  Now that you know you can do linear equations, can you show us something more complex?\n",
      "[1089.36s -> 1092.40s]  I think so. Okay, next let's try something coding related.\n",
      "[1093.04s -> 1095.36s]  So let me take out a computer for this.\n",
      "[1100.24s -> 1105.28s]  Okay, so I'm going to open the ChatGBT desktop app like Mira was talking about before.\n",
      "[1107.84s -> 1109.92s]  Okay, and to give a bit of background of what's going on.\n",
      "[1109.92s -> 1113.44s]  So here we have a computer and on the screen, we have some code.\n",
      "[1113.44s -> 1115.76s]  And then the ChatGBT voice app is on the right.\n",
      "[1115.76s -> 1119.60s]  So ChatGBT will be able to hear me, but it can't see anything on the screen.\n",
      "[1119.60s -> 1123.52s]  So I'm going to highlight the code, command see it, and then that will send it to ChatGBT.\n",
      "[1123.52s -> 1125.36s]  And then I'm going to talk about the code to ChatGBT.\n",
      "[1126.80s -> 1127.68s]  Hey ChatGBT.\n",
      "[1129.12s -> 1130.40s]  Hey there, how's it going?\n",
      "[1131.04s -> 1134.40s]  Yeah, it's going really well. I was wondering if you could help me with a coding problem today.\n",
      "[1135.28s -> 1139.28s]  Of course, I'd love to help you out. What's the coding problem you're dealing with?\n",
      "[1139.92s -> 1142.08s]  Okay, I'm going to share with you some code one second.\n",
      "[1142.96s -> 1144.32s]  Sure thing. Take your time.\n",
      "[1147.60s -> 1150.00s]  Okay, so I just shared some code with you.\n",
      "[1150.00s -> 1152.96s]  Could you give me a really brief one sentence description of what's going on in the code?\n",
      "[1153.84s -> 1158.80s]  This code fetches daily weather data for a specific location and time period,\n",
      "[1158.80s -> 1162.00s]  smooths the temperature data using a rolling average,\n",
      "[1162.00s -> 1165.44s]  annotates a significant weather event on the resulting plot,\n",
      "[1165.44s -> 1170.24s]  and then displays the plot with the average minimum and maximum temperatures over the year.\n",
      "[1171.12s -> 1174.56s]  Awesome, thanks so much. And in the code, there's this function foo.\n",
      "[1174.56s -> 1178.08s]  Can you describe to me what the plot would look like if I had versus didn't have this function?\n",
      "[1178.96s -> 1185.92s]  Sure. The function foo xy takes in a data frame x containing temperature data\n",
      "[1185.92s -> 1189.36s]  and a window size y for calculating the rolling mean.\n",
      "[1189.92s -> 1195.68s]  It smooths the average temperature, minimum temperature, and maximum temperature columns\n",
      "[1195.68s -> 1199.36s]  by applying a rolling mean over the specified window.\n",
      "[1199.36s -> 1203.84s]  With the foo function, the plot would show smoother temperature lines\n",
      "[1203.84s -> 1208.56s]  because the rolling mean reduces the noise or fluctuations in the data.\n",
      "[1208.56s -> 1212.32s]  Awesome, thanks so much. So yeah, I definitely will want to apply this function.\n",
      "[1212.32s -> 1215.68s]  And so I'm going to generate the plot and then show it to you. Give me one second.\n",
      "[1216.00s -> 1220.56s]  Absolutely. Take your time. I'm excited to see the plot when you're ready.\n",
      "[1221.12s -> 1225.20s]  Yeah, so so far ChatGPT has only been able to see the code, and now I'm going to run it,\n",
      "[1225.20s -> 1228.72s]  and then I'll use the vision capabilities of the ChatGPT desktop app\n",
      "[1228.72s -> 1230.80s]  so it can actually see everything going on on the screen as well.\n",
      "[1237.92s -> 1240.80s]  Okay, ChatGPT, I'm sharing with you the plot now.\n",
      "[1240.80s -> 1243.84s]  I'm wondering if you can give me a really brief one-sentence overview of what you see.\n",
      "[1245.76s -> 1252.16s]  The plot displays smoothed average, minimum, and maximum temperatures throughout 2018,\n",
      "[1252.16s -> 1256.48s]  with a notable annotation marking a big rainfall event in late September.\n",
      "[1257.76s -> 1261.44s]  Fantastic, I have another question for you. Which months do you see the hottest\n",
      "[1261.44s -> 1264.00s]  temperatures and roughly what temperatures do those correspond to?\n",
      "[1265.44s -> 1269.12s]  The hottest temperatures occur around July and August.\n",
      "[1269.76s -> 1276.16s]  The maximum temperature during these months is roughly between 25 degrees and 30 degrees,\n",
      "[1276.16s -> 1279.84s]  77 degrees Fahrenheit to 86 degrees Fahrenheit.\n",
      "[1280.72s -> 1285.04s]  What's going on with the y-axis? Is this in Celsius or Fahrenheit?\n",
      "[1285.68s -> 1290.24s]  The y-axis is in Celsius. The temperatures are labeled accordingly,\n",
      "[1290.24s -> 1294.40s]  such as average temperature, minimum temperature, and maximum temperature.\n",
      "[1294.96s -> 1302.08s]  Yeah, so as we can see, not only can ChatGPT help me solve very easy linear algebra equations\n",
      "[1302.08s -> 1306.64s]  that I need help with, it can also interact with code bases and see the outputs of plots\n",
      "[1306.64s -> 1308.08s]  and everything like this going on on a computer.\n",
      "[1315.84s -> 1320.48s]  So a few minutes ago, we asked a live audience on X to submit\n",
      "[1320.48s -> 1325.76s]  a few requests for what they'd like us to try out here. So I will take a couple of prompts.\n",
      "[1328.16s -> 1335.20s]  Okay, Bot Gascar wants to know if GPT4O is capable of real-time translation.\n",
      "[1336.16s -> 1338.32s]  Mike, you want to try this one? Sure, yeah, let's do it.\n",
      "[1338.32s -> 1341.84s]  All right, I speak Italian, so we can just do English and Italian.\n",
      "[1341.84s -> 1343.68s]  Let's try to do English and Italian. Sure, let's do it.\n",
      "[1346.24s -> 1347.60s]  Hey, ChatGPT, how are you?\n",
      "[1348.56s -> 1352.40s]  I'm doing fantastic. Thanks for asking. How about you? How's your day going?\n",
      "[1352.40s -> 1355.76s]  Well, I'm doing great. So I would like you to function as a translator.\n",
      "[1355.76s -> 1359.52s]  I have a friend here who only speaks Italian, and I only speak English,\n",
      "[1359.52s -> 1362.80s]  and every time you hear English, I want you to translate it to Italian,\n",
      "[1362.80s -> 1365.76s]  and if you hear Italian, I want you to translate it back to English. Is that good?\n",
      "[1370.80s -> 1375.76s]  Mike, I wonder if the whales could speak, what would they say?\n",
      "[1376.72s -> 1380.96s]  Mike, she wonders if whales could talk, what would they tell us?\n",
      "[1382.08s -> 1384.56s]  They might ask, how do we solve linear equations?\n",
      "[1396.48s -> 1399.28s]  Certainly, yes. Great, looks like it works.\n",
      "[1399.28s -> 1409.44s]  All right, so the next one is from Burrito John 78,\n",
      "[1409.44s -> 1414.24s]  who asks, can you tell what you're feeling just by looking at your face?\n",
      "[1415.04s -> 1418.16s]  Barrett, do you want to give this one a try? Absolutely, let's try it out.\n",
      "[1421.76s -> 1427.20s]  Hey, ChatGPT. Hey there, what's up? How can I brighten your day today?\n",
      "[1427.20s -> 1430.00s]  Okay, yeah, so I'm going to show you a selfie of what I look like,\n",
      "[1430.00s -> 1433.36s]  and then I'd like you to try to see what emotions I'm feeling based on how I'm looking.\n",
      "[1434.32s -> 1437.76s]  Sounds like a fun challenge. Go ahead and show me that selfie,\n",
      "[1437.76s -> 1440.32s]  and I'll put my emotional detective hat on.\n",
      "[1440.88s -> 1444.64s]  Okay, so here's me. So what kind of emotions do you think I'm feeling?\n",
      "[1446.64s -> 1450.16s]  Hmm, it seems like I'm looking at a picture of a wooden surface.\n",
      "[1450.16s -> 1454.00s]  Oh, you know what? That was the thing I sent you before. Don't worry, I'm not actually a table.\n",
      "[1454.48s -> 1458.56s]  Okay, so take another look. That makes more sense.\n",
      "[1461.76s -> 1466.32s]  There we go. It looks like you're feeling pretty happy and cheerful,\n",
      "[1466.32s -> 1469.36s]  with a big smile and maybe even a touch of excitement.\n",
      "[1469.92s -> 1472.80s]  Whatever's going on, it seems like you're in a great mood.\n",
      "[1473.52s -> 1475.68s]  Care to share the source of those good vibes?\n",
      "[1475.68s -> 1478.64s]  Yeah, no, the reason I'm in a really good mood is because we were doing a presentation\n",
      "[1478.64s -> 1480.48s]  showcasing how useful and amazing you are.\n",
      "[1480.48s -> 1484.96s]  Oh, stop it. You're making me blush.\n",
      "[1489.44s -> 1492.72s]  Amazing. Well, that's it for today on live demos.\n",
      "[1493.68s -> 1498.32s]  As you can see, this just feels so magical and that's wonderful,\n",
      "[1498.32s -> 1502.96s]  but we also want to remove some of the mysticism from the technology\n",
      "[1502.96s -> 1505.84s]  and bring it to you so you can try it for yourself.\n",
      "[1505.84s -> 1510.08s]  So over the next few weeks, we'll be rolling out these capabilities to everyone.\n",
      "[1510.80s -> 1516.48s]  Today has been very much focused on the free users and the new modalities and new products,\n",
      "[1516.48s -> 1519.52s]  but we also care a lot about the next frontier.\n",
      "[1520.24s -> 1525.04s]  So soon, we'll be updating you on our progress towards the next big thing.\n",
      "[1526.32s -> 1530.64s]  And before we wrap up, I just want to thank the incredible OpenAI team\n",
      "[1531.28s -> 1536.56s]  and also thanks to Jensen and the NVIDIA team for bringing us the most advanced GPUs\n",
      "[1536.56s -> 1542.72s]  to make this demo possible today. And thank you all very, very much for being a part of this today.\n",
      "[1566.56s -> 1572.72s]  Thank you.\n",
      "CPU times: user 6min 49s, sys: 2min 58s, total: 9min 48s\n",
      "Wall time: 6min 48s\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "\n",
    "language, segments = transcribe(audio_extract)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SubRip(SRT)\n",
    "# Subtitle Index : 0, 1, 2, //\n",
    "# Timecode : Start and end markers. HH:MM:SS,sss format. \n",
    "# Text : The subtitle text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is a helper function that takes time in seconds and \n",
    "# converts it into HH:MM:SS,sss format for the SRT subtitle files. \n",
    "\n",
    "\n",
    "\n",
    "def format_time_for_srt(seconds):\n",
    "    hours = math.floor(seconds / 3600)\n",
    "    seconds %= 3600\n",
    "    minutes = math.floor(seconds / 60)\n",
    "    seconds %=60\n",
    "    milliseconds = round((seconds - math.floor(seconds)) * 1000)\n",
    "    seconds = math.floor(seconds)\n",
    "    formatted_time = f\"{hours :02d}:{minutes:02d}:{seconds:02d},{milliseconds:03d}\"\n",
    "\n",
    "    return formatted_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_subtitle_file(input_file, language, segments):\n",
    "    subtitle_file = f\"sub-{input_file}.{language}.srt\"\n",
    "    text = \"\"\n",
    "    for index, segment in enumerate(segments):\n",
    "        segment_start = format_time_for_srt(segment.start)\n",
    "        segment_end = format_time_for_srt(segment.end)\n",
    "\n",
    "        text += f\"{str(index + 1)}\\n\"\n",
    "        text += f\"{segment_start} --> {segment_end}\\n\"\n",
    "        text += f\"{segment.text}\\n\\n\"\n",
    "    \n",
    "    f = open(subtitle_file, \"w\")\n",
    "    f.write(text)\n",
    "    f.close()\n",
    "\n",
    "    return subtitle_file\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "subtitle_file = generate_subtitle_file(yt.title, language, segments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_subtitle_to_video(input_file, subtitle_file, subtitle_language):\n",
    "    video_input_stream = ffmpeg.input(input_file)\n",
    "    subtitle_input_stream = ffmpeg.input(subtitle_file)\n",
    "    output_video = f\"output-{input_file}-{subtitle_language}.mp4\"\n",
    "    subtitle_track_tile = subtitle_file.replace(\".srt\",\"\")\n",
    "    stream = ffmpeg.output(video_input_stream, output_video,\n",
    "                           vf = f\"subtitles={subtitle_file}\")\n",
    "    ffmpeg.run(stream, overwrite_output=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'en'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "language"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ffmpeg version 7.0 Copyright (c) 2000-2024 the FFmpeg developers\n",
      "  built with Apple clang version 15.0.0 (clang-1500.3.9.4)\n",
      "  configuration: --prefix=/opt/homebrew/Cellar/ffmpeg/7.0_1 --enable-shared --enable-pthreads --enable-version3 --cc=clang --host-cflags= --host-ldflags='-Wl,-ld_classic' --enable-ffplay --enable-gnutls --enable-gpl --enable-libaom --enable-libaribb24 --enable-libbluray --enable-libdav1d --enable-libharfbuzz --enable-libjxl --enable-libmp3lame --enable-libopus --enable-librav1e --enable-librist --enable-librubberband --enable-libsnappy --enable-libsrt --enable-libssh --enable-libsvtav1 --enable-libtesseract --enable-libtheora --enable-libvidstab --enable-libvmaf --enable-libvorbis --enable-libvpx --enable-libwebp --enable-libx264 --enable-libx265 --enable-libxml2 --enable-libxvid --enable-lzma --enable-libfontconfig --enable-libfreetype --enable-frei0r --enable-libass --enable-libopencore-amrnb --enable-libopencore-amrwb --enable-libopenjpeg --enable-libopenvino --enable-libspeex --enable-libsoxr --enable-libzmq --enable-libzimg --disable-libjack --disable-indev=jack --enable-videotoolbox --enable-audiotoolbox --enable-neon\n",
      "  libavutil      59.  8.100 / 59.  8.100\n",
      "  libavcodec     61.  3.100 / 61.  3.100\n",
      "  libavformat    61.  1.100 / 61.  1.100\n",
      "  libavdevice    61.  1.100 / 61.  1.100\n",
      "  libavfilter    10.  1.100 / 10.  1.100\n",
      "  libswscale      8.  1.100 /  8.  1.100\n",
      "  libswresample   5.  1.100 /  5.  1.100\n",
      "  libpostproc    58.  1.100 / 58.  1.100\n",
      "Input #0, mov,mp4,m4a,3gp,3g2,mj2, from 'Introducing GPT-4o':\n",
      "  Metadata:\n",
      "    major_brand     : mp42\n",
      "    minor_version   : 0\n",
      "    compatible_brands: isommp42\n",
      "    creation_time   : 2024-05-13T17:40:47.000000Z\n",
      "    encoder         : Google\n",
      "  Duration: 00:26:12.64, start: 0.000000, bitrate: 827 kb/s\n",
      "  Stream #0:0[0x1](und): Video: h264 (High) (avc1 / 0x31637661), yuv420p(tv, bt709, progressive), 1280x720 [SAR 1:1 DAR 16:9], 695 kb/s, 30 fps, 30 tbr, 15360 tbn (default)\n",
      "      Metadata:\n",
      "        creation_time   : 2024-05-13T17:40:47.000000Z\n",
      "        handler_name    : ISO Media file produced by Google Inc. Created on: 05/13/2024.\n",
      "        vendor_id       : [0][0][0][0]\n",
      "  Stream #0:1[0x2](und): Audio: aac (LC) (mp4a / 0x6134706D), 44100 Hz, stereo, fltp, 128 kb/s (default)\n",
      "      Metadata:\n",
      "        creation_time   : 2024-05-13T17:40:47.000000Z\n",
      "        handler_name    : ISO Media file produced by Google Inc. Created on: 05/13/2024.\n",
      "        vendor_id       : [0][0][0][0]\n",
      "[Parsed_subtitles_0 @ 0x136628650] libass API version: 0x1701000\n",
      "[Parsed_subtitles_0 @ 0x136628650] libass source: tarball: 0.17.1\n",
      "[Parsed_subtitles_0 @ 0x136628650] Shaper: FriBidi 1.0.13 (SIMPLE) HarfBuzz-ng 8.4.0 (COMPLEX)\n",
      "[Parsed_subtitles_0 @ 0x136628650] Using font provider coretext\n",
      "Stream mapping:\n",
      "  Stream #0:0 -> #0:0 (h264 (native) -> h264 (libx264))\n",
      "  Stream #0:1 -> #0:1 (aac (native) -> aac (native))\n",
      "Press [q] to stop, [?] for help\n",
      "[Parsed_subtitles_0 @ 0x136782670] libass API version: 0x1701000\n",
      "[Parsed_subtitles_0 @ 0x136782670] libass source: tarball: 0.17.1\n",
      "[Parsed_subtitles_0 @ 0x136782670] Shaper: FriBidi 1.0.13 (SIMPLE) HarfBuzz-ng 8.4.0 (COMPLEX)\n",
      "[Parsed_subtitles_0 @ 0x136782670] Using font provider coretext\n",
      "[Parsed_subtitles_0 @ 0x136782670] fontselect: (Arial, 400, 0) -> /System/Library/Fonts/Supplemental/Arial.ttf, -1, ArialMT\n",
      "[libx264 @ 0x136627ee0] using SAR=1/1\n",
      "[libx264 @ 0x136627ee0] using cpu capabilities: ARMv8 NEON\n",
      "[libx264 @ 0x136627ee0] profile High, level 3.1, 4:2:0, 8-bit\n",
      "[libx264 @ 0x136627ee0] 264 - core 164 r3108 31e19f9 - H.264/MPEG-4 AVC codec - Copyleft 2003-2023 - http://www.videolan.org/x264.html - options: cabac=1 ref=3 deblock=1:0:0 analyse=0x3:0x113 me=hex subme=7 psy=1 psy_rd=1.00:0.00 mixed_ref=1 me_range=16 chroma_me=1 trellis=1 8x8dct=1 cqm=0 deadzone=21,11 fast_pskip=1 chroma_qp_offset=-2 threads=15 lookahead_threads=2 sliced_threads=0 nr=0 decimate=1 interlaced=0 bluray_compat=0 constrained_intra=0 bframes=3 b_pyramid=2 b_adapt=1 b_bias=0 direct=1 weightb=1 open_gop=0 weightp=2 keyint=250 keyint_min=25 scenecut=40 intra_refresh=0 rc_lookahead=40 rc=crf mbtree=1 crf=23.0 qcomp=0.60 qpmin=0 qpmax=69 qpstep=4 ip_ratio=1.40 aq=1:1.00\n",
      "Output #0, mp4, to 'output-Introducing GPT-4o-en.mp4':\n",
      "  Metadata:\n",
      "    major_brand     : mp42\n",
      "    minor_version   : 0\n",
      "    compatible_brands: isommp42\n",
      "    encoder         : Lavf61.1.100\n",
      "  Stream #0:0(und): Video: h264 (avc1 / 0x31637661), yuv420p(tv, bt709, progressive), 1280x720 [SAR 1:1 DAR 16:9], q=2-31, 30 fps, 15360 tbn (default)\n",
      "      Metadata:\n",
      "        creation_time   : 2024-05-13T17:40:47.000000Z\n",
      "        handler_name    : ISO Media file produced by Google Inc. Created on: 05/13/2024.\n",
      "        vendor_id       : [0][0][0][0]\n",
      "        encoder         : Lavc61.3.100 libx264\n",
      "      Side data:\n",
      "        cpb: bitrate max/min/avg: 0/0/0 buffer size: 0 vbv_delay: N/A\n",
      "  Stream #0:1(und): Audio: aac (LC) (mp4a / 0x6134706D), 44100 Hz, stereo, fltp, 128 kb/s (default)\n",
      "      Metadata:\n",
      "        creation_time   : 2024-05-13T17:40:47.000000Z\n",
      "        handler_name    : ISO Media file produced by Google Inc. Created on: 05/13/2024.\n",
      "        vendor_id       : [0][0][0][0]\n",
      "        encoder         : Lavc61.3.100 aac\n",
      "[out#0/mp4 @ 0x13662fd10] video:127075KiB audio:24656KiB subtitle:0KiB other streams:0KiB global headers:0KiB muxing overhead: 1.097672%\n",
      "frame=47179 fps=319 q=-1.0 Lsize=  153397KiB time=00:26:12.56 bitrate= 799.1kbits/s speed=10.6x    \n",
      "[libx264 @ 0x136627ee0] frame I:208   Avg QP:16.38  size: 87548\n",
      "[libx264 @ 0x136627ee0] frame P:12280 Avg QP:20.47  size:  6339\n",
      "[libx264 @ 0x136627ee0] frame B:34691 Avg QP:27.66  size:   982\n",
      "[libx264 @ 0x136627ee0] consecutive B-frames:  1.2%  1.5%  2.0% 95.3%\n",
      "[libx264 @ 0x136627ee0] mb I  I16..4: 15.7% 51.1% 33.2%\n",
      "[libx264 @ 0x136627ee0] mb P  I16..4:  1.0%  2.8%  0.6%  P16..4: 15.8%  4.4%  2.7%  0.0%  0.0%    skip:72.8%\n",
      "[libx264 @ 0x136627ee0] mb B  I16..4:  0.1%  0.2%  0.0%  B16..8: 14.1%  0.7%  0.2%  direct: 0.3%  skip:84.4%  L0:45.0% L1:52.5% BI: 2.5%\n",
      "[libx264 @ 0x136627ee0] 8x8 transform intra:60.6% inter:72.2%\n",
      "[libx264 @ 0x136627ee0] coded y,uvDC,uvAC intra: 50.8% 57.6% 23.9% inter: 2.4% 2.3% 0.2%\n",
      "[libx264 @ 0x136627ee0] i16 v,h,dc,p: 41% 29% 10% 19%\n",
      "[libx264 @ 0x136627ee0] i8 v,h,dc,ddl,ddr,vr,hd,vl,hu: 27% 20% 24%  3%  5%  6%  5%  5%  5%\n",
      "[libx264 @ 0x136627ee0] i4 v,h,dc,ddl,ddr,vr,hd,vl,hu: 37% 22% 12%  4%  6%  6%  5%  4%  4%\n",
      "[libx264 @ 0x136627ee0] i8c dc,h,v,p: 48% 23% 22%  8%\n",
      "[libx264 @ 0x136627ee0] Weighted P-Frames: Y:0.2% UV:0.1%\n",
      "[libx264 @ 0x136627ee0] ref P L0: 64.4% 12.5% 16.3%  6.8%  0.0%\n",
      "[libx264 @ 0x136627ee0] ref B L0: 88.3%  9.7%  1.9%\n",
      "[libx264 @ 0x136627ee0] ref B L1: 95.1%  4.9%\n",
      "[libx264 @ 0x136627ee0] kb/s:661.94\n",
      "[aac @ 0x13677b020] Qavg: 651.169\n"
     ]
    }
   ],
   "source": [
    "add_subtitle_to_video(yt.title, subtitle_file, language)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
